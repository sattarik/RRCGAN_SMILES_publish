{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "opposite-jordan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' fix all the seeds,results are still slighthly different '"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Strategy 1:\n",
    "# Generate data after each epoch of training, if less than\n",
    "# 10% error rate, and is a legit SMILES\n",
    "# append to the real data\n",
    "# Otherwise, append to fake data\n",
    "\n",
    "# ADDING REINFORCEMENT MECHANISM\n",
    "# Regenerate Normal sampling (define ranges), default: uniform\n",
    "\n",
    "# IMPORTANT!!!!!!!!!!!!! DO NOT DROP DUPLICATE FOR RESULT .CSV\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from numpy import ndarray\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error \n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import explained_variance_score\n",
    "\n",
    "\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import (Input, Dropout, LSTM, Reshape, LeakyReLU,\n",
    "                          Concatenate, ReLU, Flatten, Dense, Embedding,\n",
    "                          BatchNormalization, Activation, SpatialDropout1D,\n",
    "                          Conv2D, MaxPooling2D, UpSampling2D, Lambda)\n",
    "from tensorflow.keras.models     import Model, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses     import mse, binary_crossentropy\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "from tensorflow.keras.metrics import  mean_squared_error as mse_keras\n",
    "from tensorflow.keras.backend import argmax as argmax\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow import one_hot\n",
    "import np_utils\n",
    "from tensorflow.keras.utils import  to_categorical\n",
    "from tensorflow import random as randomtf\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.lines as mlines\n",
    "from   matplotlib.lines import Line2D\n",
    "from   matplotlib.colors import ListedColormap\n",
    "import matplotlib.ticker as tk\n",
    "\n",
    "from progressbar import ProgressBar\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from chainer_chemistry.dataset.preprocessors import GGNNPreprocessor, construct_atomic_number_array\n",
    "preprocessor = GGNNPreprocessor()\n",
    "from rdkit import rdBase\n",
    "rdBase.DisableLog('rdApp.error')\n",
    "from rdkit import Chem\n",
    "\n",
    "import ntpath\n",
    "from scipy.stats import truncnorm\n",
    "\n",
    "\"\"\" fix all the seeds,results are still slighthly different \"\"\"\n",
    "#randomtf.set_seed(1)\n",
    "#os.environ['PYTHONHASHSEED'] = '0'\n",
    "#np.random.seed(42)\n",
    "#random.seed(12345)\n",
    "#gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.3667)\n",
    "#session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1, gpu_options=gpu_options)\n",
    "#tf.set_random_seed(1234)\n",
    "#sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "#K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "intensive-hybrid",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './../data/trainingsets/60000_train_regular_qm9/image_train.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-cd700f45fdbb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\"\"\" reading and preprocessing data\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./../data/trainingsets/60000_train_regular_qm9/image_train.pickle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mX_smiles_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSMILES_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_atoms_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_bonds_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./../data/trainingsets/60000_train_regular_qm9/image_test.pickle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './../data/trainingsets/60000_train_regular_qm9/image_train.pickle'"
     ]
    }
   ],
   "source": [
    "\"\"\" reading and preprocessing data\"\"\"\n",
    "with open('./../data/trainingsets/60000_train_regular_qm9/image_train.pickle', 'rb') as f:\n",
    "    X_smiles_train, SMILES_train, X_atoms_train, X_bonds_train, y_train = pickle.load(f)\n",
    "    \n",
    "with open('./../data/trainingsets/60000_train_regular_qm9/image_test.pickle', 'rb') as f:\n",
    "    X_smiles_val, SMILES_val, X_atoms_val, X_bonds_val, y_val = pickle.load(f)\n",
    "\n",
    "with open('./../data/trainingsets/60000_train_regular_qm9/tokenizer.pickle', 'rb') as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "tokenizer[0] = ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sublime-treaty",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Subsampling has been done in the data preprocesses\n",
    "print ('X_smiles_train shape: ', X_smiles_train.shape)\n",
    "print ('X_smiles_test shape: ', X_smiles_val.shape)\n",
    "#print ('last SMILES train: ', SMILES_train[-1])\n",
    "\n",
    "## Outlier removal 1.5*IQR rule\n",
    "# Train samples\n",
    "IQR = - np.quantile(y_train, 0.25) + np.quantile(y_train, 0.75)\n",
    "lower_bound, upper_bound = np.quantile(y_train, 0.25) - 1.5 * IQR, np.quantile(y_train, 0.75) + 1.5 * IQR\n",
    "idx = np.where((y_train >= lower_bound) & (y_train <= upper_bound))\n",
    "\n",
    "y_train = y_train[idx]\n",
    "X_smiles_train = X_smiles_train[idx]\n",
    "X_atoms_train = X_atoms_train[idx]\n",
    "X_bonds_train = X_bonds_train[idx]\n",
    "\n",
    "# Test samples\n",
    "IQR = - np.quantile(y_val, 0.25) + np.quantile(y_val, 0.75)\n",
    "lower_bound, upper_bound = np.quantile(y_val, 0.25) - 1.5 * IQR, np.quantile(y_val, 0.75) + 1.5 * IQR\n",
    "idx = np.where((y_val >= lower_bound) & (y_val <= upper_bound))\n",
    "\n",
    "y_val = y_val[idx]\n",
    "X_smiles_val = X_smiles_val[idx]\n",
    "X_atoms_val = X_atoms_val[idx]\n",
    "X_bonds_val = X_bonds_val[idx]\n",
    "\n",
    "# normalize the bond and aotm matrices:\n",
    "def norm(X: ndarray) -> ndarray:\n",
    "    X = np.where(X == 0, -1.0, 1.0)\n",
    "    return X\n",
    "\n",
    "X_atoms_train, X_bonds_train = (norm(X_atoms_train),\n",
    "                                norm(X_bonds_train))\n",
    "X_atoms_val, X_bonds_val = (norm(X_atoms_val),\n",
    "                            norm(X_bonds_val))\n",
    "# normalize the property\n",
    "s_min1 = np.min (y_train)\n",
    "s_max1 = np.max (y_train)\n",
    "\n",
    "s_min2 = np.min(y_val)\n",
    "s_max2 = np.max(y_val)\n",
    "\n",
    "s_min = min(s_min1, s_min2)\n",
    "s_max = max(s_max1, s_max2)\n",
    "\n",
    "s_min_dataset, s_max_dataset = s_min, s_max\n",
    "s_min_norm, s_max_norm = s_min_dataset, s_max_dataset\n",
    "\n",
    "y_val   = (y_val -   s_min_norm) / (s_max_norm - s_min_norm)\n",
    "y_train = (y_train - s_min_norm) / (s_max_norm - s_min_norm)\n",
    "print (\"min and max dataset and val normalized\", s_min, s_max, np.min(y_val), np.max(y_val))\n",
    "print (\"min and max dataset and train normalized\", s_min, s_max, np.min(y_train), np.max(y_train))\n",
    "print (\"min and max used for normalization: \", s_min_norm, s_max_norm)\n",
    "\n",
    "\"\"\" models definition and extracting pretrained encoder and decoder \"\"\"\n",
    "encoder = load_model('./../data/nns_9HA_noemb_6b6/encoder_newencinp.h5')\n",
    "decoder = load_model('./../data/nns_9HA_noemb_6b6/decoder_newencinp.h5')\n",
    "\n",
    "class Config:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.Filters = [256, 128, 64]\n",
    "        self.genFilters = [128, 128, 128]\n",
    "        self.upFilters = [(2, 2), (2, 2), (2, 2)]\n",
    "        \n",
    "config = Config()\n",
    "\n",
    "## Generator \n",
    "z = Input(shape = (128, ))\n",
    "y = Input(shape = (1, ))\n",
    "\n",
    "h = Concatenate(axis = 1)([z, y])\n",
    "h = Dense(1 * 1 * 128)(h)\n",
    "R1 = Reshape([1, 1, 128])(h)\n",
    "R2 = Reshape([1, 1, 128])(h)\n",
    "\n",
    "for i in range(3):\n",
    "    R1 = UpSampling2D(size = config.upFilters[i])(R1)\n",
    "    C1 = Conv2D(filters = config.genFilters[i], \n",
    "               kernel_size = 2, \n",
    "               strides = 1, \n",
    "               padding = 'same')(R1)\n",
    "    B1 = BatchNormalization()(C1)\n",
    "    R1 = LeakyReLU(alpha = 0.2)(B1)\n",
    "\n",
    "for i in range(3):\n",
    "    R2 = UpSampling2D(size = config.upFilters[i])(R2)\n",
    "    C2 = Conv2D(filters = config.genFilters[i], \n",
    "               kernel_size = 2, \n",
    "               strides = 1, \n",
    "               padding = 'same')(R2)\n",
    "    B2 = BatchNormalization()(C2)\n",
    "    R2 = LeakyReLU(alpha = 0.2)(B2)\n",
    "    \n",
    "R1 = Conv2D(1,\n",
    "            kernel_size = 3,\n",
    "            strides = 1,\n",
    "            padding = 'valid',\n",
    "            activation = 'tanh')(R1)\n",
    "R2 = Conv2D(1,\n",
    "            kernel_size = 3,\n",
    "            strides = 1,\n",
    "            padding = 'valid',\n",
    "            activation = 'tanh')(R2)\n",
    "\n",
    "generator = Model([z, y], [R1, R2])\n",
    "print (generator.summary())\n",
    "\n",
    "## Discriminator \n",
    "inp1 = Input(shape = [6, 6, 1])\n",
    "inp2 = Input(shape = [6, 6, 1])\n",
    "\n",
    "X1 = Concatenate()([inp1, inp2])\n",
    "X = Flatten()(X1)\n",
    "y2 = Concatenate(axis = 1)([X, y])\n",
    "for i in range(3):\n",
    "\t\ty2 = Dense(64, activation = 'relu')(y2)\n",
    "\t\ty2 = LeakyReLU(alpha = 0.2)(y2)\n",
    "\t\ty2 = Dropout(0.2)(y2)\n",
    "\n",
    "O_dis = Dense(1, activation = 'sigmoid')(y2)\n",
    "\n",
    "\n",
    "discriminator = Model([inp1, inp2, y], O_dis)\n",
    "discriminator.compile(loss = 'binary_crossentropy', optimizer = Adam(lr = 5e-6, beta_1 = 0.5))\n",
    "print (discriminator.summary()) \n",
    "\n",
    "## Regressor\n",
    "inp1 = Input(shape = [6, 6, 1])\n",
    "inp2 = Input(shape = [6, 6, 1])\n",
    "\n",
    "yr = Concatenate()([inp1, inp2])\n",
    "\n",
    "tower0 = Conv2D(32, 1, padding = 'same')(yr)\n",
    "tower1 = Conv2D(64, 1, padding = 'same')(yr)\n",
    "tower1 = Conv2D(64, 3, padding = 'same')(tower1)\n",
    "tower2 = Conv2D(32, 1, padding = 'same')(yr)\n",
    "tower2 = Conv2D(32, 5, padding = 'same')(tower2)\n",
    "tower3 = MaxPooling2D(3, 1, padding = 'same')(yr)\n",
    "tower3 = Conv2D(32, 1, padding = 'same')(tower3)\n",
    "h = Concatenate()([tower0, tower1, tower2, tower3])\n",
    "h = ReLU()(h)\n",
    "h = MaxPooling2D(2, 1, padding = 'same')(h)\n",
    "\n",
    "for i in range(6):\n",
    "    tower0 = Conv2D(32, 1, padding = 'same')(h)\n",
    "    tower1 = Conv2D(64, 1, padding = 'same')(h)\n",
    "    tower1 = Conv2D(64, 3, padding = 'same')(tower1)\n",
    "    tower2 = Conv2D(32, 1, padding = 'same')(h)\n",
    "    tower2 = Conv2D(32, 5, padding = 'same')(tower2)\n",
    "    tower3 = MaxPooling2D(3, 1, padding = 'same')(h)\n",
    "    tower3 = Conv2D(32, 1, padding = 'same')(tower3)\n",
    "    h = Concatenate()([tower0, tower1, tower2, tower3])\n",
    "    h = ReLU()(h)\n",
    "    if i % 2 == 0 and i != 0:\n",
    "        h = MaxPooling2D(2, 1, padding = 'same')(h)\n",
    "h = BatchNormalization()(h)\n",
    "\n",
    "yr = Flatten()(h)\n",
    "o = Dropout(0.2)(yr)\n",
    "o = Dense(128)(o)\n",
    "\n",
    "o_reg = Dropout(0.2)(o)\n",
    "o_reg = Dense(1, activation = 'sigmoid')(o_reg)\n",
    "\n",
    "regressor = Model([inp1, inp2], o_reg)\n",
    "regressor_top = Model([inp1, inp2], o)\n",
    "\n",
    "# Training the Regressor \n",
    "# latent vectors from trained Encoder, \n",
    "# last output of Encoder is concat. (O1, O2)\n",
    "train_atoms_embedding, train_bonds_embedding, _ = encoder.predict([X_smiles_train])\n",
    "atoms_embedding, bonds_embedding, _ = encoder.predict([X_smiles_train])\n",
    "atoms_val, bonds_val, _ = encoder.predict([X_smiles_val])\n",
    "\n",
    "try:\n",
    "    regressor =     load_model('./../data/nns_9HA_noemb_6b6/regressor.h5')\n",
    "    regressor_top = load_model('./../data/nns_9HA_noemb_6b6/regressor_top.h5')\n",
    "    print (\".h5 was read\")\n",
    "except:\n",
    "    print (\"no .h5 available\")\n",
    "    regressor.compile(loss = 'mse', optimizer = Adam(1e-5))\n",
    "    pass\n",
    "\"\"\"\n",
    "history = regressor.fit([atoms_embedding, bonds_embedding], \n",
    "              y_train,\n",
    "              validation_data = ([atoms_val,\n",
    "                                  bonds_val],\n",
    "                                 y_val),\n",
    "              batch_size = 128,\n",
    "              epochs = 1,\n",
    "              verbose = 1)\n",
    "\"\"\"\n",
    "# keep test data unseen\n",
    "history = regressor.fit([atoms_embedding, bonds_embedding],\n",
    "              y_train,\n",
    "              batch_size = 128,\n",
    "              epochs = 1,\n",
    "              verbose = 1)\n",
    "\n",
    "# Validating the regressor\n",
    "# Train\n",
    "pred = regressor.predict([atoms_embedding, bonds_embedding])\n",
    "print('Current R2 on Regressor for train data: {}'.format(r2_score(y_train, pred.reshape([-1]))))\n",
    "mse_train_normalized = mean_squared_error(y_train, pred.reshape([-1]))\n",
    "mse_train = mse_train_normalized * ((s_max_norm - s_min_norm)**2)\n",
    "print ('norm. train MSE: ', mse_train_normalized, ', train MSE: ', mse_train)\n",
    "print ('norm. train RMSE: ', mse_train_normalized**0.5, ', train RMSE: ', mse_train**0.5)\n",
    "print ('prediction on train: ', pred)\n",
    "print ('True train: ', y_train)\n",
    "# Test\n",
    "pred = regressor.predict([atoms_val, bonds_val])\n",
    "print('Current R2 on Regressor for test data: {}'.format(r2_score(y_val, pred.reshape([-1]))))\n",
    "mse_test_normalized = mean_squared_error(y_val, pred.reshape([-1]))\n",
    "mse_test = mse_test_normalized * ((s_max_norm - s_min_norm)**2)\n",
    "print ('norm. test MSE: ', mse_test_normalized, 'test MSE: ', mse_test)\n",
    "print ('norm. test RMSE: ', mse_test_normalized**0.5, 'test RMSE: ', mse_test**0.5)\n",
    "print (\"prediction on test: \", pred )\n",
    "print (\"True test values: \", y_val)\n",
    "\n",
    "# Saving the currently trained models\n",
    "regressor.save('./../data/nns_9HA_noemb_6b6/regressor.h5')\n",
    "regressor_top.save('./../data/nns_9HA_noemb_6b6/regressor_top.h5')\n",
    "\n",
    "\"\"\"\n",
    "# save the losses \n",
    "with open ('regressor_loss_100_150.csv', 'w') as f:\n",
    "    for key in history.history.keys():\n",
    "        f.write(\"%s,%s\\n\"%(key,history.history[key]))\n",
    "\n",
    "# plot history for loss\n",
    "plt.close()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('Regressor loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.savefig(\"R_loss_0_100.png\", dpi=300)\n",
    "\"\"\"\n",
    "\n",
    "## Combined model \n",
    "def build_combined(z, y,\n",
    "                   regressor,\n",
    "                   regressor_top,\n",
    "                   discriminator,\n",
    "                   encoder,\n",
    "                   decoder):\n",
    "    discriminator.trainable = False\n",
    "    regressor_top.trainable = False\n",
    "    regressor.trainable = False\n",
    "    encoder.trainable = False\n",
    "    decoder.trainable = False\n",
    "    \n",
    "    atoms_emb, bonds_emb = generator([z, y])\n",
    "    dec_embedding = Concatenate()([atoms_emb, bonds_emb])\n",
    "    \n",
    "    softmax_smiles, _ = decoder([dec_embedding])\n",
    "    argmax_smiles = argmax (softmax_smiles, axis=2)\n",
    "    argmax_smiles = Reshape([35])(argmax_smiles)\n",
    "    smiles = one_hot(argmax_smiles, depth=23)\n",
    "    smiles = Reshape([35, 23, 1])(smiles)\n",
    "    latent_encoder_atom, latent_encoder_bond, _ = encoder ([smiles])\n",
    "    \n",
    "    y_pred = regressor([latent_encoder_atom, latent_encoder_bond])\n",
    "    valid = discriminator([atoms_emb, bonds_emb, y])\n",
    "\n",
    "    combined = Model([z, y], [valid, y_pred])\n",
    "\n",
    "    combined.compile(loss = ['binary_crossentropy',\n",
    "                             'mse'], \n",
    "                     loss_weights = [1.0, 25.0], \n",
    "                     optimizer = Adam(5e-6, beta_1 = 0.5))\n",
    "    return combined\n",
    "\n",
    "combined = build_combined(z, y,\n",
    "                          regressor,\n",
    "                          regressor_top,\n",
    "                          discriminator,\n",
    "                          encoder,\n",
    "                          decoder)\n",
    "\n",
    "\"\"\" Training RCGAN \"\"\"\n",
    "# loading pretrained models\n",
    "regressor = load_model    ('./../data/nns_9HA_noemb_6b6/regressor.h5')\n",
    "regressor_top = load_model('./../data/nns_9HA_noemb_6b6/regressor_top.h5')\n",
    "generator = load_model    ('./../data/nns_9HA_noemb_6b6/generator.h5')\n",
    "discriminator= load_model ('./../data/nns_9HA_noemb_6b6/discriminator.h5')\n",
    "\n",
    "regressor_top.trainable = False\n",
    "regressor.trainable = False\n",
    "\n",
    "# SMILES related information\n",
    "max_gen_atoms = 9\n",
    "bond_max = 9\n",
    "MAX_NB_WORDS = 23\n",
    "MAX_SEQUENCE_LENGTH = 35\n",
    "\"\"\"\n",
    "epochs = 500\n",
    "batch_size = 64\n",
    "batches = y_train.shape[0] // batch_size\n",
    "threshold = 0.2 # defining accurate samples\n",
    "reinforce_n = 50 # 5*reinforce_n = fake sampling\n",
    "reinforce_sample = 1000 # how many samples generated for Reinforcement\n",
    "\n",
    "# variable for storing generated data\n",
    "G_Losses = []\n",
    "D_Losses = []\n",
    "R_Losses = []\n",
    "D_Losses_real = []\n",
    "D_Losses_fake = []\n",
    "\n",
    "for e in range(epochs):\n",
    "    start = time.time()\n",
    "    D_loss = []\n",
    "    G_loss = []\n",
    "    R_loss = []\n",
    "    D_loss_real = []\n",
    "    D_loss_fake = []\n",
    "    \n",
    "    for b in range(batches):\n",
    "        \n",
    "        regressor_top.trainable = False\n",
    "        regressor.trainable = False\n",
    "\n",
    "        idx = np.arange(b * batch_size, (b + 1) * batch_size)\n",
    "        # rearrange the samples \n",
    "        idx = np.random.choice(idx, batch_size, replace = False)\n",
    "        \n",
    "        x_smiles_train = X_smiles_train[idx] \n",
    "        batch_y = y_train[idx]\n",
    "        \n",
    "        batch_z = np.random.normal(0, 1, size = (batch_size, 128))\n",
    "        \n",
    "        atoms_embedding, bonds_embedding, _ = encoder.predict([x_smiles_train])\n",
    "        dec_embedding = np.concatenate([atoms_embedding, bonds_embedding], axis = -1)\n",
    "        \n",
    "        gen_atoms_embedding, gen_bonds_embedding = generator.predict([batch_z, batch_y])\n",
    "        \n",
    "        gen_dec_embedding = np.concatenate([gen_atoms_embedding, gen_bonds_embedding], axis = -1)\n",
    "        softmax_smiles = decoder.predict(gen_dec_embedding)[0]\n",
    "        argmax_smiles = np.argmax(softmax_smiles, axis = 2)\n",
    "        smiles = to_categorical(argmax_smiles, num_classes=23)\n",
    "        SHAPE = list(smiles.shape) + [1]\n",
    "        smiles = smiles.reshape(SHAPE)\n",
    "        latent_encoder_atom, latent_encoder_bond, _ = encoder.predict([smiles])\n",
    "        gen_pred = regressor.predict([latent_encoder_atom, latent_encoder_bond]).reshape([-1])\n",
    "        \n",
    "        regressor.trainable = True\n",
    "        r_loss = regressor.train_on_batch([atoms_embedding, bonds_embedding], batch_y)\n",
    "        R_loss.append(r_loss)\n",
    "        regressor.trainable = False\n",
    "\n",
    "        discriminator.trainable = True\n",
    "        # original was 3!\n",
    "        if b<100:\n",
    "            d=1\n",
    "        else:\n",
    "            d=3\n",
    "        for _ in range(d):\n",
    "            d_loss_real = discriminator.train_on_batch([atoms_embedding,bonds_embedding, batch_y],\n",
    "                                                       [0.9 * np.ones((batch_size, 1))])\n",
    "            d_loss_fake = discriminator.train_on_batch([gen_atoms_embedding,gen_bonds_embedding, gen_pred],\n",
    "                                                       [np.zeros((batch_size, 1))])\n",
    "\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "        D_loss.append(d_loss)\n",
    "        D_loss_real.append (d_loss_real)\n",
    "        D_loss_fake.append (d_loss_fake)\n",
    "        discriminator.trainable = False\n",
    "        \n",
    "        regressor_top.trainable = False\n",
    "        regressor.trainable = False\n",
    "\n",
    "        g_loss = combined.train_on_batch([batch_z, batch_y], [0.9 * np.ones((batch_size, 1)), batch_y])\n",
    "        G_loss.append(g_loss)\n",
    "    \n",
    "    D_Losses.append(np.mean(D_loss))\n",
    "    D_Losses_real.append(np.mean(D_loss_real))\n",
    "    D_Losses_fake.append(np.mean(D_loss_fake))\n",
    "    G_Losses.append(np.mean(G_loss))\n",
    "    R_Losses.append(np.mean(R_loss))\n",
    "    \n",
    "    print('====')\n",
    "    print('Current epoch: {}/{}'.format((e + 1), epochs))\n",
    "    print ('D Loss Real: {}'.format(np.mean(D_loss_real)))\n",
    "    print ('D Loss Fake: {}'.format(np.mean(D_loss_fake)))\n",
    "    print('D Loss: {}'.format(np.mean(D_loss)))\n",
    "    print('G Loss: {}'.format(np.mean(G_loss)))\n",
    "    print('R Loss: {}'.format(np.mean(R_loss)))\n",
    "    print('====')\n",
    "    print()\n",
    "\n",
    "    \n",
    "    # Reinforcement\n",
    "    gen_error = []\n",
    "    gen_smiles = []\n",
    "    gen_valid_smiles = []\n",
    "    gen_X_atoms = []\n",
    "    gen_X_bonds = []\n",
    "    predcv_AE_latent = []\n",
    "    embeddings = []\n",
    "    sample_ys = []\n",
    "    valid_smiles_index = []\n",
    "    for _ in range(reinforce_sample):\n",
    "        sample_y = np.random.uniform(s_min_dataset, s_max_dataset, size = [1,])\n",
    "        sample_y = np.round(sample_y, 4)\n",
    "        sample_y = (sample_y - s_min_norm) / (s_max_norm - s_min_norm)\n",
    "        sample_ys.append(sample_y)\n",
    "\n",
    "        sample_z = np.random.normal(0, 1, size = (1, 128))\n",
    "\n",
    "        sample_atoms_embedding, sample_bonds_embedding = generator.predict([sample_z, sample_y])\n",
    "        embeddings.append((sample_atoms_embedding, sample_bonds_embedding))\n",
    "        \n",
    "        dec_embedding = np.concatenate([sample_atoms_embedding, sample_bonds_embedding], axis = -1)\n",
    "        softmax_smiles = decoder.predict(dec_embedding)[0]\n",
    "        argmax_smiles = np.argmax(softmax_smiles, axis = 2).reshape([-1])\n",
    "        smiles = to_categorical(argmax_smiles, num_classes=23)\n",
    "        SHAPE = [1] + list(smiles.shape) + [1]\n",
    "        smiles = smiles.reshape(SHAPE)\n",
    "        c_smiles = ''\n",
    "        for s in argmax_smiles:\n",
    "            c_smiles += tokenizer[s]\n",
    "        c_smiles = c_smiles.rstrip()\n",
    "        \n",
    "        if _==0:\n",
    "            #print (\"Gen. sample Reinforce center from Decoder\", smiles)\n",
    "            print ('\"   \"   converted to SMILES\"',c_smiles)\n",
    "        gen_smiles.append(c_smiles)\n",
    "        latent_encoder_atom, latent_encoder_bond, _ = encoder.predict([smiles])\n",
    "        reg_pred = regressor.predict([latent_encoder_atom, latent_encoder_bond])\n",
    "        \n",
    "        pred, desire = reg_pred[0][0], sample_y[0]\n",
    "        gen_error.append(np.abs((pred - desire) / desire))\n",
    "\n",
    "        \n",
    "    gen_error = np.asarray(gen_error)\n",
    "    # two validity defined: \n",
    "    # sanitizing and less than 9HA: valid, \n",
    "    # without sanitizing: valid 0    \n",
    "    valid = 0\n",
    "    valid0 = 0\n",
    "    idx_ = []\n",
    "    idx0_ = []\n",
    "    for iter_, smiles in enumerate(gen_smiles):\n",
    "        if ' ' in smiles[:-1]:\n",
    "            continue\n",
    "        m  = Chem.MolFromSmiles(smiles[:-1], sanitize=True)\n",
    "        m0 = Chem.MolFromSmiles(smiles[:-1], sanitize=False)\n",
    "        if m0 is not None:\n",
    "            valid0 += 1\n",
    "            idx0_.append(iter_)\n",
    "        if m is not None:\n",
    "            if len(construct_atomic_number_array(m)) <= 9:\n",
    "                valid += 1\n",
    "                idx_.append(iter_)\n",
    "                try:\n",
    "                    gen_smiles [iter_] = Chem.MolToSmiles(m)\n",
    "                    print (Chem.MolToSmiles(m))\n",
    "                    print (\"Hc_des\", sample_ys[iter_])\n",
    "                    print (\"error\", gen_error[iter_])\n",
    "                except:\n",
    "                    pass\n",
    "    idx_ = np.asarray(idx_)\n",
    "    idx0_ = np.asarray(idx0_)\n",
    "\n",
    "    validity = [gen_smiles[jj] for jj in idx0_ ]\n",
    "    validity = pd.DataFrame(validity)\n",
    "    validity = validity.drop_duplicates()\n",
    "\n",
    "    validity_sanitize = [gen_smiles[jj] for jj in idx_ ]\n",
    "    validity_sanitize = pd.DataFrame(validity_sanitize)\n",
    "    validity_sanitize = validity_sanitize.drop_duplicates()\n",
    "\n",
    "    if (e + 1) % 100 == 0:\n",
    "        reinforce_n += 10\n",
    "\n",
    "    # invalid smiles:\n",
    "    fake_indices1 = np.setdiff1d(np.arange(reinforce_sample), np.asarray(idx_))\n",
    "    fake_indices2 = np.intersect1d(np.where(gen_error > threshold)[0], idx_)\n",
    "    fake_indices = np.concatenate ((fake_indices1, fake_indices2))\n",
    "    fake_indices = np.random.choice(fake_indices, reinforce_n * 5, replace = False)\n",
    "\n",
    "    real_indices_ = np.intersect1d(np.where(gen_error < threshold)[0], idx_)\n",
    "    sample_size =  len(real_indices_)\n",
    "    real_indices = np.random.choice(real_indices_, sample_size, replace = False)\n",
    "    \n",
    "    # Activating Reinforcement \n",
    "    if e >= 5:\n",
    "        discriminator.trainable = True\n",
    "        regressor_top.trainable = False\n",
    "        regressor.trainable = False\n",
    "        for real_index in real_indices:\n",
    "            #real_latent = regressor_top.predict([embeddings[real_index][0], embeddings[real_index][1]])\n",
    "            _ = discriminator.train_on_batch([embeddings[real_index][0], embeddings[real_index][1], sample_ys[real_index]],\n",
    "                                             [0.9 * np.ones((1, 1))])\n",
    "\n",
    "        for fake_index in fake_indices:\n",
    "            #fake_latent = regressor_top.predict([embeddings[fake_index][0], embeddings[fake_index][1]])\n",
    "            _ = discriminator.train_on_batch([embeddings[fake_index][0], embeddings[fake_index][1] , sample_ys[fake_index]],\n",
    "                                             [np.zeros((1, 1))])\n",
    "        discriminator.trainable = False\n",
    "\n",
    "    # ==== #\n",
    "    try:\n",
    "        print('Currently valid SMILES (No chemical_beauty and sanitize off): {}'.format(valid0))\n",
    "        print('Currently valid SMILES Unique (No chemical_beauty and sanitize off): {}'.format(len(validity)))\n",
    "        print('Currently valid SMILES Sanitized: {}'.format(valid))\n",
    "        print('Currently valid Unique SMILES Sanitized: {}'.format(len(validity_sanitize)))\n",
    "        print('Currently satisfying SMILES: {}'.format(len(real_indices_)))\n",
    "        print('Currently unique satisfying generation: {}'.format(len(np.unique(np.array(gen_smiles)[real_indices_]))))\n",
    "        print('Gen Sample is: {}, for {}'.format(c_smiles, sample_y))\n",
    "        print('Predicted val: {}'.format(reg_pred))\n",
    "        print('====')\n",
    "        print()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    if (e + 1) % 5 == 0:\n",
    "        plt.close()\n",
    "        fig, ax = plt.subplots(figsize = (12, 10))\n",
    "        ax.tick_params(axis='both', which='major', labelsize=30)\n",
    "        plt.plot(G_Losses, color='blue')\n",
    "        plt.plot(D_Losses, color='red')\n",
    "        plt.xlabel('epochs', fontsize=35)\n",
    "        plt.ylabel('loss', fontsize=35)\n",
    "        mpl.rcParams['axes.linewidth'] = 2.5\n",
    "        #plt.plot(R_Losses)\n",
    "        plt.legend(['G Loss', 'D Loss'], fontsize=30)\n",
    "        plt.savefig(\"G_D_losses{}.png\".format (e+1))\n",
    "    n_unique = len(np.unique(np.array(gen_smiles)[real_indices_]))\n",
    "    n_valid = valid\n",
    "    if valid > 450 and n_unique > 350:\n",
    "        print('Criteria has satisified, training has ended')\n",
    "        break\n",
    "\n",
    "    end = time.time()\n",
    "    print (\"time for current epoch: \", (end - start))\n",
    "\n",
    "\n",
    "with open('GAN_loss.pickle', 'wb') as f:\n",
    "    pickle.dump((G_Losses, D_Losses, R_Losses), f)\n",
    "\n",
    "# Saving the currently trained models\n",
    "#regressor.save('regressor.h5')\n",
    "#regressor_top.save('regressor_top.h5')\n",
    "generator.save('./../data/nns_9HA_noemb_6b6/generator_new.h5')\n",
    "discriminator.save('./../data/nns_9HA_noemb_6b6/discriminator_new.h5')\n",
    "\n",
    "##====#\n",
    "\"\"\"\n",
    "# Generation Study\n",
    "\n",
    "#regressor = load_model('regressor.h5')\n",
    "#regressor_top = load_model('regressor_top.h5')\n",
    "#generator = load_model    ('./../data/nns_9HA_noemb_6b6/generator_new.h5')\n",
    "#discriminator = load_model('./../data/nns_9HA_noemb_6b6/discriminator_new.h5')\n",
    "\n",
    "encoder = load_model('./../data/nns_9HA_noemb_6b6/encoder_newencinp.h5')\n",
    "decoder = load_model('./../data/nns_9HA_noemb_6b6/decoder_newencinp.h5')\n",
    "\n",
    "# Generation workflow\n",
    "# 1. Given a desired heat capacity\n",
    "# 2. Generate 10,000 samples of SMILES embedding\n",
    "# 3. Select the ones with small relative errors (< 10%)\n",
    "# 4. Transfer them to SMILES\n",
    "# 5. Filter out the invalid SMILES\n",
    "\n",
    "# Generate 500 different values of heat capacities\n",
    "\n",
    "from progressbar import ProgressBar\n",
    "\n",
    "N = 2000\n",
    "n_sample = 100\n",
    "\n",
    "gen_error = []\n",
    "gen_smiles = []\n",
    "sample_ys = []\n",
    "preds = []\n",
    "gen_atoms_embedding = []\n",
    "gen_bonds_embedding = []\n",
    "\n",
    "regressor_top.trainable = False\n",
    "regressor.trainable = False\n",
    "generator.trainable = False\n",
    "discriminator.trainable = False\n",
    "\n",
    "pbar = ProgressBar()\n",
    "for hc in pbar(range(n_sample)):\n",
    "    #try:\n",
    "        # get it back to original of s_min to s_max\n",
    "        sample_y = np.random.uniform(s_min_dataset, s_max_dataset, size=[1,])\n",
    "        #X = get_truncated_normal(mean=30, sd=5, low=s_min, upp=s_max)\n",
    "        #sample_y = X.rvs()\n",
    "        print (sample_y)\n",
    "        sample_y = np.round(sample_y, 4)\n",
    "        sample_y = sample_y * np.ones([N,])\n",
    "        sample_y_ = (sample_y - s_min_norm) / (s_max_norm - s_min_norm)\n",
    "        sample_z = np.random.normal(0, 1, size = (N, 128))\n",
    "\n",
    "        regressor_top.trainable = False\n",
    "        regressor.trainable = False\n",
    "\n",
    "        sample_atoms_embedding, sample_bonds_embedding = generator.predict([sample_z, sample_y_])\n",
    "        dec_embedding = np.concatenate([sample_atoms_embedding, sample_bonds_embedding], axis = -1)\n",
    "        \n",
    "        softmax_smiles = decoder.predict(dec_embedding)[0]\n",
    "        argmax_smiles = np.argmax(softmax_smiles, axis = 2)\n",
    "        print ('shape argmax_smiles', argmax_smiles.shape)\n",
    "        smiles = to_categorical(argmax_smiles, num_classes=23)\n",
    "        SHAPE = list(smiles.shape) + [1] \n",
    "        print ('shape line 767', SHAPE) \n",
    "        smiles = smiles.reshape(SHAPE)\n",
    "        \n",
    "        latent_encoder_atom, latent_encoder_bond, _ = encoder.predict([smiles])\n",
    "        pred = regressor.predict([latent_encoder_atom, latent_encoder_bond]).reshape([-1])\n",
    "        pred = pred * (s_max_norm - s_min_norm) + s_min_norm\n",
    "\n",
    "        gen_errors = np.abs((pred - sample_y) / sample_y).reshape([-1])\n",
    "\n",
    "        #accurate = np.where(gen_errors <= 0.2)[0]\n",
    "        #gen_errors = gen_errors[accurate]\n",
    "        #pred = pred[accurate]\n",
    "\n",
    "        #sample_y = sample_y[accurate]\n",
    "        #sample_atoms_embedding = sample_atoms_embedding[accurate]\n",
    "        #sample_bonds_embedding = sample_bonds_embedding[accurate]\n",
    "\n",
    "        smiles = decoder.predict(dec_embedding)[0]\n",
    "        smiles = np.argmax(smiles, axis = 2).reshape(smiles.shape[0], 35)\n",
    "\n",
    "        generated_smiles = []\n",
    "        for S in smiles:\n",
    "            c_smiles = ''\n",
    "            for s in S:\n",
    "                c_smiles += tokenizer[s]\n",
    "            c_smiles = c_smiles.rstrip()\n",
    "            generated_smiles.append(c_smiles)\n",
    "        generated_smiles = np.array(generated_smiles)\n",
    "        #generated_smiles = generated_smiles [accurate]\n",
    "        all_gen_smiles = []\n",
    "        idx = []\n",
    "        for i, smiles in enumerate(generated_smiles):\n",
    "            all_gen_smiles.append(smiles[:-1])\n",
    "\n",
    "            if ' ' in smiles[:-1]:\n",
    "                continue\n",
    "            #m = Chem.MolFromSmiles(smiles[:-1],sanitize=False)\n",
    "            m = Chem.MolFromSmiles(smiles[:-1])\n",
    "            if m is not None:\n",
    "                if len(construct_atomic_number_array(m)) <= 9:\n",
    "                    idx.append(i)\n",
    "\n",
    "        idx = np.array(idx)\n",
    "        all_gen_smiles = np.array(all_gen_smiles)\n",
    "        print ('all gen smiels shape', all_gen_smiles.shape)\n",
    "        print ('gen_errors shape', gen_errors.shape)\n",
    "        gen_smiles.extend(list(all_gen_smiles[idx]))\n",
    "        gen_error.extend(list(gen_errors[idx]))\n",
    "        sample_ys.extend(list(sample_y[idx]))\n",
    "        gen_atoms_embedding.extend(sample_atoms_embedding[idx])\n",
    "        gen_bonds_embedding.extend(sample_bonds_embedding[idx])\n",
    "\n",
    "        preds.extend(list(pred[idx]))\n",
    "    #except:\n",
    "    #    print('Did not discover SMILES for HC: {}'.format(sample_y))\n",
    "    #    pass    \n",
    "\n",
    "\n",
    "output = {}\n",
    "\n",
    "for i, s in enumerate (gen_smiles):\n",
    "    ss = Chem.MolToSmiles(Chem.MolFromSmiles(s))\n",
    "    gen_smiles[i] = ss\n",
    "\n",
    "output['SMILES'] = gen_smiles\n",
    "output['des_cv'] = sample_ys\n",
    "output['pred_cv'] = preds\n",
    "output['Err_pred_des'] = gen_error\n",
    "\n",
    "plt.close()\n",
    "plt.hist(gen_error)\n",
    "plt.savefig(\"gen_error_hist.png\")\n",
    "\n",
    "output = pd.DataFrame(output)\n",
    "output.reset_index(drop = True, inplace = True)\n",
    "output.to_csv ('./../experiments/regular_9HA_6b6latent/Regular_20screen2.csv', index=False)\n",
    "## Statistics  (# DFT=True value, Des=prediction)\n",
    "\n",
    "# total # of samples\n",
    "N = len(gen_error)\n",
    "# Explained Variance R2 from sklearn.metrics.explained_variance_score\n",
    "explained_variance_R2_DFT_des = explained_variance_score(output['pred_cv'], output['des_cv'])\n",
    "print (\"explained_varice_R2_DFT_des\", explained_variance_R2_DFT_des)\n",
    "\n",
    "# mean absolute error \n",
    "MAE_DFT_des = mean_absolute_error(output['pred_cv'], output['des_cv'])\n",
    "print (\"MAE_DFT_des\", MAE_DFT_des)\n",
    "# Fractioned MAE, more normalized\n",
    "Fractioned_MAE_DFT_des = 0\n",
    "for dft, des in zip(output['pred_cv'], output['des_cv']):\n",
    "    Fractioned_MAE_DFT_des = Fractioned_MAE_DFT_des +  abs(des-dft)/des\n",
    "Fractioned_MAE_DFT_des = Fractioned_MAE_DFT_des/N\n",
    "print (\"Fractioned MAE_DFT_des\", Fractioned_MAE_DFT_des)\n",
    "\n",
    "# root mean squared error (RMSE), sqrt(sklearn ouputs MSE)\n",
    "RMSE_DFT_des = mean_squared_error(output['pred_cv'], output['des_cv'])**0.5\n",
    "print (\"RMSE_DFT_des\", RMSE_DFT_des)\n",
    "\n",
    "Fractioned_RMSE_DFT_des = 0\n",
    "for dft, des in zip(output['pred_cv'], output['des_cv']):\n",
    "    Fractioned_RMSE_DFT_des = Fractioned_RMSE_DFT_des + ((des-dft)/des)**2\n",
    "Fractioned_RMSE_DFT_des = (Fractioned_RMSE_DFT_des/N)**0.5\n",
    "print (\"Fractioned_RMSE_DFT_des\", Fractioned_RMSE_DFT_des)\n",
    "\n",
    "#output = pd.DataFrame(output)\n",
    "# do not drop duplicate\n",
    "output2 = output.drop_duplicates(['SMILES'])\n",
    "#gen_atoms_embedding = np.array(gen_atoms_embedding)\n",
    "#gen_bonds_embedding = np.array(train_atoms_embedding)\n",
    "\n",
    "\"\"\"\n",
    "# ANALYSIS\n",
    "X_atoms_train_ = train_atoms_embedding.reshape([train_atoms_embedding.shape[0], \n",
    "                                        6 * 6])\n",
    "X_bonds_train_ = train_bonds_embedding.reshape([train_bonds_embedding.shape[0], \n",
    "                                        6 * 6])\n",
    "\n",
    "X_atoms_test_ = gen_atoms_embedding.reshape([gen_atoms_embedding.shape[0],\n",
    "                                      6 * 6])\n",
    "X_bonds_test_ = gen_bonds_embedding.reshape([gen_bonds_embedding.shape[0], \n",
    "                                      6 * 6])\n",
    "\n",
    "pca_1 = PCA(n_components = 2)\n",
    "X_atoms_train_ = pca_1.fit_transform(X_atoms_train_)\n",
    "X_atoms_test_ = pca_1.transform(X_atoms_test_)\n",
    "\n",
    "pca_2 = PCA(n_components = 2)\n",
    "X_bonds_train_ = pca_2.fit_transform(X_bonds_train_)\n",
    "X_bonds_test_ = pca_2.transform(X_bonds_test_)\n",
    "\n",
    "# Atoms Distribution\n",
    "plt.close()\n",
    "plt.scatter(X_atoms_train_[:,0], X_atoms_train_[:,1], alpha = 0.3, c = 'blue');\n",
    "plt.savefig(\"train_atom_dist.png\")\n",
    "#plt.close()\n",
    "plt.scatter(X_atoms_test_[:,0], X_atoms_test_[:,1], alpha = 0.3, c = 'red');\n",
    "plt.savefig(\"test_atom_dist.png\")\n",
    "####\n",
    "\n",
    "# Bonds Distribution\n",
    "plt.close()\n",
    "plt.scatter(X_bonds_train_[:,0], X_bonds_train_[:,1], alpha = 0.3, c = 'blue');\n",
    "plt.savefig(\"train_bonds_dist.png\")\n",
    "#plt.close()\n",
    "plt.scatter(X_bonds_test_[:,0], X_bonds_test_[:,1], alpha = 0.3, c = 'red');\n",
    "plt.savefig(\"test_bonds_dist.png\")\n",
    "# 31/500 failed (N = 10000)\n",
    "# 2/50 failed (N = 50000)\n",
    "\"\"\"\n",
    "#output.reset_index(drop = True, inplace = True)\n",
    "output2.reset_index(drop = True, inplace = True)\n",
    "#output.to_csv ('./../experiments/regular_9HA_6b6latent/Regular_noscreen.csv', index = False)\n",
    "output2.to_csv('./../experiments/regular_9HA_6b6latent/Regular_NODUP_20screen2.csv', index = False)\n",
    "\"\"\"with open('gen_pickles.pickle', 'wb') as f:\n",
    "    pickle.dump(gen_unique_pickles, f)\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
