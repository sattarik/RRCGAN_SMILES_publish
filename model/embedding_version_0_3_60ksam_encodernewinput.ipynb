{"cells":[{"cell_type":"code","execution_count":null,"id":"brown-bottle","metadata":{"id":"brown-bottle","outputId":"06c65e78-5a2b-4f14-d2bc-12c468e93cb0"},"outputs":[{"name":"stdout","output_type":"stream","text":["tf version 2.4.1\n"]}],"source":["# -*- coding: utf-8 -*-\n","\"\"\"Untitled0.ipynb\n","\n","Automatically generated by Colaboratory.\n","\n","Original file is located at\n","    https://colab.research.google.com/drive/1jeBcxYNVbeeknPCjl9IAp8ZOp07vtRqu\n","\"\"\"\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","import os\n","import pandas as pd\n","import tensorflow as tf\n","\n","from tensorflow import keras\n","import random\n","import numpy as np\n","from numpy import ndarray\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import CountVectorizer\n","import pickle\n","\n","from tensorflow.keras.layers import (Input, Dropout, LSTM, Reshape, LeakyReLU,\n","                          Concatenate, ReLU, Flatten, Dense, Embedding,\n","                          BatchNormalization, Activation, SpatialDropout1D,\n","                          Conv2D, MaxPooling2D, Softmax, \n","                           Lambda)\n","#from tensorflow.keras.layers.experimental.preprocessing import CategoryEncoding\n","from tensorflow.keras.models import Model, load_model\n","from tensorflow.keras.optimizers import Adam, RMSprop\n","import tensorflow.keras.backend as K\n","from tensorflow.keras.activations import tanh\n","\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","#import np_utils\n","from tensorflow.keras.utils import to_categorical\n","from IPython.display import clear_output\n","import matplotlib.pyplot as plt\n","import csv\n","from progressbar import ProgressBar\n","import seaborn as sns\n","from tensorflow import random as randomtf\n","from tensorflow.keras.backend import argmax as argmax\n","\n","from tensorflow import one_hot\n","\n","randomtf.set_seed(1)\n","os.environ['PYTHONHASHSEED'] = '0'\n","np.random.seed(42)\n","random.seed(12345)\n","#gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)\n","#session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1, gpu_options=gpu_options)\n","#tf.set_random_seed(1234)\n","#sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n","#K.set_session(sess)\n","\n","print ('tf version', tf.__version__)"]},{"cell_type":"code","execution_count":null,"id":"bored-enterprise","metadata":{"id":"bored-enterprise"},"outputs":[],"source":["# Commented out IPython magic to ensure Python compatibility.\n","# %cd ./drive/MyDrive/RCGAN_SMILES/RCGAN_SMILES/\n","\n","with open('./../data/trainingsets/60000_train_regular_qm9/image_train.pickle', 'rb') as f:\n","    X_smiles_train, SMILES_train, X_atoms_train, X_bonds_train, y_train = pickle.load(f)\n","\n","with open('./../data/trainingsets/60000_train_regular_qm9/image_test.pickle', 'rb') as f:\n","    X_smiles_test, SMILES_test, X_atoms_test, X_bonds_test, y_test = pickle.load(f)\n","\n","# Outlier removal train\n","IQR = - np.quantile(y_train, 0.25) + np.quantile(y_train, 0.75)\n","\n","lower_bound, upper_bound = np.quantile(y_train, 0.25) - 1.5 * IQR, np.quantile(y_train, 0.75) + 1.5 * IQR\n","\n","idx = np.where((y_train >= lower_bound) & (y_train <= upper_bound))\n","\n","y_train = y_train[idx]\n","X_smiles_train = X_smiles_train[idx]\n","X_atoms_train = X_atoms_train[idx]\n","X_bonds_train = X_bonds_train[idx]\n","SMILES_train = SMILES_train [idx]\n","# Subsampling has been done in the data preprocesses\n","print ('X_smiles_train shape: ', X_smiles_train.shape)\n","print ('X_smiles_test shape: ', X_smiles_test.shape)\n","print ('X_smiles_train first sample: ', X_smiles_train[0][:][:][:])\n","print ('First SMILES train: ', SMILES_train[0])\n","print ('first cv', y_train[0])\n","# Outlier removal test\n","IQR = - np.quantile(y_test, 0.25) + np.quantile(y_test, 0.75)\n","\n","lower_bound, upper_bound = np.quantile(y_test, 0.25) - 1.5 * IQR, np.quantile(y_test, 0.75) + 1.5 * IQR\n","\n","idx = np.where((y_test >= lower_bound) & (y_test <= upper_bound))\n","\n","y_test = y_test[idx]\n","X_smiles_test = X_smiles_test[idx]\n","X_atoms_test = X_atoms_test[idx]\n","X_bonds_test = X_bonds_test[idx]\n","SMILES_test = SMILES_test [idx]\n","def norm(X: ndarray) -> ndarray:\n","    X = np.where(X == 0, -1.0, 1.0)\n","    return X\n","\n","X_atoms_train, X_bonds_train = (norm(X_atoms_train),\n","                                norm(X_bonds_train))\n","\n","X_atoms_test, X_bonds_test = (norm(X_atoms_test),\n","                              norm(X_bonds_test))\n","\n","X_smiles_train.shape\n","\n","# Normalize y_train, y_test\n","s_min1 = np.min (y_train)\n","s_max1 = np.max (y_train)\n","\n","s_min2 = np.min(y_test)\n","s_max2 = np.max(y_test)\n","\n","# Use these values for generation.\n","s_min_dataset = min(s_min1, s_min2)\n","s_max_dataset = max(s_max1, s_max2)\n","s_min_norm, s_max_norm = s_min_dataset, s_max_dataset\n","\n","y_train = (y_train - s_min_norm) / (s_max_norm - s_min_norm)\n","y_test = (y_test - s_min_norm) / (s_max_norm -   s_min_norm)\n","\n","print (\"min and max used to normalized\", s_min_norm, s_max_norm) \n","print (\"min and max of this data testing after norm\", np.min(y_test), np.max(y_test))\n","print (\"min and max of this data training after norm\", np.min(y_train), np.max(y_train))\n","print (\"min and max of dataset to be used for generation\", s_min_dataset, s_max_dataset)\n","\n","# Encoding to an image embedding\n","\n","# ENCODER\n","#inp_1 = Input(shape = [9, 10, 1])\n","#inp_2 = Input(shape = [9, 9, 4])\n","inp_1 = Input(shape = [35, 23, 1])\n","\n","y1 = Conv2D(64, (15, 3), strides = 1, padding = 'valid')(inp_1)\n","y1 = LeakyReLU(alpha = 0.2)(y1)\n","y1 = BatchNormalization()(y1)\n","\n","y1 = Conv2D(64, 6, strides = 1, padding = 'valid')(y1)\n","y1 = LeakyReLU(alpha = 0.2)(y1)\n","y1 = BatchNormalization()(y1)\n","\n","y1 = Conv2D(64, 6, strides = 1, padding = 'valid')(y1)\n","y1 = LeakyReLU(alpha = 0.2)(y1)\n","y1 = BatchNormalization()(y1)\n","\n","y1 = Conv2D(64, 6, strides = 1, padding = 'valid')(y1)\n","y1 = LeakyReLU(alpha = 0.2)(y1)\n","y1 = BatchNormalization()(y1)\n","\n","y1_emb = Conv2D(1, 3, strides = 1, padding = 'same',\n","            activation = 'tanh')(y1)\n","\n","y2 = Conv2D(64, (15, 3), strides = 1, padding = 'valid')(inp_1)\n","y2 = LeakyReLU(alpha = 0.2)(y2)\n","y2 = BatchNormalization()(y2)\n","\n","y2 = Conv2D(64, 6, strides = 1, padding = 'valid')(y2)\n","y2 = LeakyReLU(alpha = 0.2)(y2)\n","y2 = BatchNormalization()(y2)\n","\n","y2 = Conv2D(64, 6, strides = 1, padding = 'valid')(y2)\n","y2 = LeakyReLU(alpha = 0.2)(y2)\n","y2 = BatchNormalization()(y2)\n","\n","y2 = Conv2D(64, 6, strides = 1, padding = 'valid')(y2)\n","y2 = LeakyReLU(alpha = 0.2)(y2)\n","y2 = BatchNormalization()(y2)\n","\n","y2_emb = Conv2D(1, 3, strides = 1, padding = 'same',\n","                activation = 'tanh')(y2)\n","\n","####\n","y_out = Concatenate()([y1_emb, y2_emb])\n","\n","# DECODER\n","emb_in = Input(shape = [6, 6, 2])\n","\n","tower0 = Conv2D(32, 1, padding = 'same')(emb_in)\n","tower1 = Conv2D(64, 1, padding = 'same')(emb_in)\n","tower1 = Conv2D(64, 3, padding = 'same')(tower1)\n","tower2 = Conv2D(32, 1, padding = 'same')(emb_in)\n","tower2 = Conv2D(32, 5, padding = 'same')(tower2)\n","tower3 = MaxPooling2D(3, 1, padding = 'same')(emb_in)\n","tower3 = Conv2D(32, 1, padding = 'same')(tower3)\n","h = Concatenate()([tower0, tower1, tower2, tower3])\n","h = ReLU()(h)\n","h = MaxPooling2D(2, 1, padding = 'same')(h)\n","\n","for i in range(6):\n","    tower0 = Conv2D(32, 1, padding = 'same')(h)\n","    tower1 = Conv2D(64, 1, padding = 'same')(h)\n","    tower1 = Conv2D(64, 3, padding = 'same')(tower1)\n","    tower2 = Conv2D(32, 1, padding = 'same')(h)\n","    tower2 = Conv2D(32, 5, padding = 'same')(tower2)\n","    tower3 = MaxPooling2D(3, 1, padding = 'same')(h)\n","    tower3 = Conv2D(32, 1, padding = 'same')(tower3)\n","    h = Concatenate()([tower0, tower1, tower2, tower3])\n","    h = ReLU()(h)\n","    if i % 2 == 0 and i != 0:\n","        h = MaxPooling2D(2, 1, padding = 'same')(h)\n","h = BatchNormalization()(h)\n","\n","y = Flatten()(h)\n","\n","y = Dense(256, activation = 'relu')(y)\n","y_cv = Dense(64, activation = 'relu')(y)\n","y = Dropout(0.2)(y)\n","y = Dense(128, activation = 'relu')(y)\n","y = Dropout(0.2)(y)\n","y = Dense(128, activation = 'relu')(y)\n","y = Dropout(0.2)(y)\n","y = Dense(35 * 23)(y)\n","y = Reshape([35, 23, 1])(y)\n","y = Softmax(axis = 2)(y)\n","\n","\n","y_cv = Dropout(0.2)(y_cv)\n","y_cv = Dense(128, activation = 'relu')(y_cv)\n","y_cv = Dropout(0.2)(y_cv)\n","y_cv = Dense(128, activation = 'relu')(y_cv)\n","y_cv = Dense(1, activation = 'sigmoid')(y_cv)\n","\n","encoder = Model([inp_1], [y1_emb, y2_emb, y_out], name = 'Encoder')\n","decoder = Model(emb_in, [y, y_cv], name = 'Decoder')\n","#print (encoder.summary())\n","#print (decoder.summary())\n","outputs = decoder(encoder([inp_1])[2])\n","#output_2 = decoder(encoder([inp_1])[2])[0]\n","#output_2 = argmax (output_2, axis=2)\n","#print (output_2)\n","#output_2 = Reshape([1, 35])(output_2)\n","\n","#print (output_2)\n","# Use IntegerLookup to build an index of the feature values and encode output.\n","#lookup = IntegerLookup(output_mode=\"one_hot\")\n","#lookup.adapt(data)\n","# Convert new test data (which includes unknown feature values)\n","#outputs = lookup(outputs)\n","#output_2 = one_hot(output_2, depth=23)\n","\n","\n","model = Model(inp_1, outputs, name = 'ae')\n","print (model.summary())\n","\n","\"\"\"\n","lr_schedule = keras.callbacks.LearningRateScheduler(\n","    lambda epoch: 1e-8 * 10 ** (epoch / 20)\n",")\n","\n","encoder = load_model('encoder.h5')\n","decoder = load_model('decoder.h5')\n","model = load_model('ae_model.h5')\n","\n","model.compile(optimizer = Adam(learning_rate = 1e-8),\n","              loss = ['binary_crossentropy', 'mse'])\n","history = model.fit([X_atoms_train, X_bonds_train],\n","                    [X_smiles_train, y_train],\n","                    validation_data = ([X_atoms_test, X_bonds_test],\n","                                       [X_smiles_test, y_test]),\n","                    epochs = 1,\n","                    batch_size = 32,\n","                    verbose = 1,\n","                    callbacks = [lr_schedule])\n","\n","\n","\n","plt.semilogx(history.history['lr'],\n","             history.history['val_Decoder_loss'])\n","\n","encoder = Model([inp_1, inp_2], [y1_emb, y2_emb, y_out], name = 'Encoder')\n","decoder = Model(emb_in, [y, y_cv], name = 'Decoder')\n","\n","print (encoder.summary())\n","print (decoder.summary())\n","\n","outputs = decoder(encoder([inp_1, inp_2])[2])\n","model = Model([inp_1, inp_2], outputs, name = 'ae')\n","\n","model.compile(optimizer = Adam(learning_rate = 9e-5),\n","              loss = ['binary_crossentropy', 'mse'])\n","\n","model.fit([X_atoms_train, X_bonds_train],\n","                    [X_smiles_train, y_train],\n","                    validation_data = ([X_atoms_test, X_bonds_test],\n","                                       [X_smiles_test, y_test]),\n","                    epochs = 1,\n","                    batch_size = 32,\n","                    verbose = 1)\n","\"\"\"\n","\n","try:\n","    encoder = load_model('./../data/nns_9HA_noemb_6b6/encoder_asuz.h5')\n","    decoder = load_model('./../data/nns_9HA_noemb_6b6/decoder_asuz.h5')\n","    model = load_model  ('./../data/nns_9HA_noemb_6b6/ae_model_asuz.h5')\n","    print (\".h5 files were read\")\n","except:\n","    print (\"NO .h5 trained files\")\n","    model.compile(optimizer = Adam(learning_rate = 9e-5),\n","              loss = ['binary_crossentropy', 'mse'])\n","    pass\n","\n","\n","\"\"\"\n","model.compile(optimizer = Adam(learning_rate = 9e-5),\n","              loss = ['binary_crossentropy', 'mse'])\n","\"\"\"\n","history = model.fit(X_smiles_train,\n","                    [X_smiles_train, y_train],\n","                    validation_data = (X_smiles_test,\n","                                       [X_smiles_test, y_test]),\n","                    epochs = 1,\n","                    batch_size = 32,\n","                    verbose = 1)\n","\n","print(history.history.keys())\n","# summarize history for loss\n","plt.close()\n","plt.plot(history.history['val_loss'])\n","plt.title('Autoencoder loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.savefig(\"autoencoderloss9HA_400_600.png\", dpi=300)\n","\n","\n","\n","model.save  ('./../data/nns_9HA_noemb_6b6/ae_model_asuz.h5')\n","encoder.save('./../data/nns_9HA_noemb_6b6/encoder_asuz.h5')\n","decoder.save('./../data/nns_9HA_noemb_6b6/decoder_asuz.h5')\n","\n","\n","\n","for i in [5, 10, 32, 88, 99]:\n","    plt.subplot(121)\n","    plt.imshow(X_smiles_train[i].reshape([35, 23]))\n","    test_sample_pred = decoder.predict(encoder.predict([X_smiles_train[i:(i+2)]])[2])[0][0]\n","    plt.subplot(122)\n","    plt.imshow(test_sample_pred.reshape([35, 23]))\n","    plt.show()\n","    plt.savefig(\"smiles_{}_train.png\".format(i)) \n","\n","# get i and i+2 to have (2,9,10,1) shape\n","# if only i was chosen, the should be (9,10,1)\n","output = decoder.predict(encoder.predict([X_smiles_train[0:2][:][:][:]])[2])[0][0]\n","output = argmax (output, axis=1)\n","output = to_categorical (output, num_classes = 23)\n","print (SMILES_train[0])\n","print (output.shape)\n","print ('output of decoder', output)\n","print (y_train[0])\n","\n","for i in [5, 10, 32, 88, 99]:\n","    plt.subplot(121)\n","    plt.imshow(X_smiles_test[i].reshape([35, 23]))\n","    test_sample_pred = decoder.predict(encoder.predict([X_smiles_test[i:(i+2)]])[2])[0][0]\n","    plt.subplot(122)\n","    plt.imshow(test_sample_pred.reshape([35, 23]))\n","    plt.show()\n","    plt.savefig(\"smiles_{}_test.png\".format(i))\n","\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.2"},"colab":{"name":"embedding_version_0_3_60ksam_encodernewinput.ipynb","provenance":[]}},"nbformat":4,"nbformat_minor":5}